=== Building a simple activity feed system

// by Travis Vachon (travis)

===== Problem

Streams are a dominant metaphor for presenting information to users of
the modern internet. Used on sites like Facebook and Twitter and mobile
apps like Instagram and Tinder, streams are an elegant tool for giving
users a window into the deluge of information generated by the
applications they use every day.

As a developer of these applications, you want tools to process the
firehose of raw event data generated by user actions. They must offer
powerful tools for filtering and aggregating data and must be
arbitrarily scalable to serve ever growing userbases. Ideally
they should provide high level abstractions that help you organize and
grow the complexity of your stream processing logic to accommodate new
features and a complex world.

Clojure offers just such a tool in Storm, a distributed realtime
computation system that aims to be for realtime computation what
Hadoop is for batch computation. In this section we'll build a simple
activity stream processing system that can be easily extended to solve
real world problems.

===== Solution

First create a new storm project using +lein+:

[source,console]
$ lein new storm-project feeds


Head into the project and run the default storm topology:

[source,console]
----
$ cd feeds
$ lein run -m feeds.topology/run!
Compiling feeds.TopologySubmitter
...
Emitting: spout default [:bizarro]
Processing received message source: spout:4, stream: default, id: {}, [:bizarro]
Emitting: stormy-bolt default ["I'm bizarro Stormy!"]
Processing received message source: stormy-bolt:5, stream: default, id: {}, [I'm bizarro Stormy!]
Emitting: feeds-bolt default ["feeds produced: I'm bizarro Stormy!"]
----

This topology just babbles incoherently, which isn't what we want, so
we'll begin by modifying the input "spout" to produce realistic
events. Open +src/feeds/spouts.clj+ and replace the +defspout+ form
with a new spout that will periodically produce random user events
that one might see in an online marketplace:

[source,clojure]
----
(defspout event-spout ["event"]
  [conf context collector]
  (let [events [{:action :commented, :user :travis, :listing :red-shoes}
                {:action :liked, :user :jim, :listing :red-shoes}
                {:action :liked, :user :karen, :listing :green-hat}
                {:action :liked, :user :rob, :listing :green-hat}
                {:action :commented, :user :emma, :listing :green-hat}]]
    (spout
     (nextTuple []
       (Thread/sleep 1000)
       (emit-spout! collector [(rand-nth events)])))))
----

Next, open +src/feeds/bolts/clj+. Add a bolt that accepts a
user and an event and produces a tuple of (user, event) for each user
in the system:

[source,clojure]
----
(defbolt active-user-bolt ["user" "event"] [{event "event" :as tuple} collector]
  (doseq [user [:jim :rob :karen :kaitlyn :emma :travis]]
    (emit-bolt! collector [user event]))
  (ack! collector tuple))
----

Now add a bolt that accepts a user and an event and emits a tuple if
and only if the user is following the user who triggered the event:

[source,clojure]
----
(defbolt follow-bolt ["user" "event"] {:prepare true}
  [conf context collector]
  (let [follows {:jim #{:rob :emma}
                 :rob #{:karen :kaitlyn :jim}
                 :karen #{:kaitlyn :emma}
                 :kaitlyn #{:jim :rob :karen :kaitlyn :emma :travis}
                 :emma #{:karen}
                 :travis #{:kaitlyn :emma :karen :rob}}]
    (bolt
     (execute [{user "user" event "event" :as tuple}]
              (when ((follows user) (:user event))
                (emit-bolt! collector [user event]))
              (ack! collector tuple)))))
----

Then add a bolt that accepts a user and an event and stores the event
in a hash of sets like +{:user1 #{event1 event2} :user2 #{event1 event2}}+ -
these are the activity streams we'll present to users.

[source,clojure]
----
(defbolt feed-bolt ["user" "event"] {:prepare true}
  [conf context collector]
  (let [feeds (atom {})]
    (bolt
     (execute [{user "user" event "event" :as tuple}]
              (swap! feeds #(update-in % [user] conj event))
              (println "Current feeds:")
              (clojure.pprint/pprint @feeds)
              (ack! collector tuple)))))
----

This gives us all the pieces we need, but we still need
to assemble them together into a computational topology. Open up
+src/feeds/topology.clj+ and use the topology DSL to wire the spouts
and bolts together:

[source,clojure]
----
(defn topology []
  (topology
   {"events" (spout-spec event-spout)}

   {"active users" (bolt-spec {"eventst" :shuffle} active-user-bolt :p 2)
    "follows" (bolt-spec {"active users" :shuffle} follow-bolt :p 2)
    "feeds" (bolt-spec {"follows" ["user"]} feed-bolt :p 2)}))
----

You'll also need to update the +:require+ statement in that file:

[source,clojure]
----
  (:require [feeds
             [spouts :refer [event-spout]]
             [bolts :refer [active-user-bolt follow-bolt feed-bolt]]]
            [backtype.storm [clojure :refer [topology spout-spec bolt-spec]] [config :refer :all]])
----

Finally, run the topology again. Feeds will be printed to the console
by the final bolts in the topology:

[source,console]
$ lein run -m feeds.topology/run!


===== Discussion

Storm's Clojure DSL doesn't look like standard Clojure - instead, it
uses Clojure's macros to extend the language to the domain of stream
processing. Storm's stream processing abstraction consists of 4 core
primitives:

- *tuples* are dynamically typed lists of values, with support for
   providing names for values
- *spouts* produce tuples, often by reading from a distributed
   queue
- *bolts* accept tuples as input and produce new tuples - these
   are the core computational units of a Storm topology
- *streams* are used to wire spouts to bolts and bolts to other bolts,
   creating a computational topology. Streams can be configured with
   rules for routing certain types of tuples to specific instances of
   bolts.

We'll review the components of our system to give a better picture of
how these primitives work together.

====== event-spout

[source,clojure]
----
(defspout event-spout ["event"]
  [conf context collector]
----

+defspout+ looks much like Clojure's standard +defn+ with one
difference - the second argument to +defspout+ is a list of names that
will be assigned to elements of each tuple this spout produces. This
lets us use tuples like vectors or maps interchangeably. The third
argument to +defspout+ is a list of arguments that will be bound
various components of Storm's operational infrastructure - we'll use
+collector+ below, but will ignore the other two for now.

[source,clojure]
----
  (let [events [{:action :commented, :user :travis, :listing :red-shoes}
                {:action :liked, :user :jim, :listing :red-shoes}
                {:action :liked, :user :karen, :listing :green-hat}
                {:action :liked, :user :rob, :listing :green-hat}
                {:action :commented, :user :emma, :listing :green-hat}]]
----

+defspout+'s body will be evaluated once, when the spout instance is
created, which gives us an opportunity to create in-memory state. In
this case we'll create a list of events this spout will produce, but
usually this will be a connection to a database or distributed queue.

[source,clojure]
----
    (spout
     (nextTuple []
       (Thread/sleep 1000)
       (emit-spout! collector [(rand-nth events)])))))
----

This call to +spout+ creates an instance of a spout with the given
implementation of +nextTuple+. This implementation simply sleeps for
one second and then uses +emit-spout!+ to emit a tuple consisting of a
random event from the list above. +nextTuple+ will be called
repeatedly in a tight loop, so if you create a spout that polls an
external resource you may need to provide your own backoff algorithm
to avoid excess load on that resource.

We can also implement the spout's +ack+ method to implement a
"reliable" spout that will provide message processing guarantees. For
more information on reliable spouts, see Storm's spout implementation
for the Kestrel queueing system.

====== active-user-bolt

Every time a user takes an action in our system we need to determine
whether every other user in the system will be interested in it. Given
a simple interest system like Twitter, where users express interest in
a single way (user follows) we could simply look at the follower list
of the user who took the action and update feeds accordingly. In a
more complex system, however, interest might be expressed by having
liked the item the action was taken against, by following a collection
that the item has been added to or by following the seller of the
item. In this world we need to consider a variety of factors for each
user in the system for each event and determine whether the event
should be added to that user's feed.

Our first bolt starts this process by generating a tuple of +(user, event)+
for each user in the system every time an event is generated by the
+event-spout+:

[source,clojure]
----
(defbolt active-user-bolt ["user" "event"] [{event "event" :as tuple} collector]
  (doseq [user [:jim :rob :karen :kaitlyn :emma :travis]]
    (emit-bolt! collector [user event]))
  (ack! collector tuple))
----

+defbolt+'s signature looks very similar to +defspout+ - the second
argument is a list of names that will be assigned to tuples generated
by this bolt, and the third argument is a list of parameters. The
first parameter will be bound to the input tuple, and may be
destructured as a map or a vector.

The body of this bolt iterates through a list of users in the system
and emits a tuple for each of them. The last line of the body +ack!+s
this tuple, which allows Storm to track message processing and restart
processing when appropriate.

====== follow-bolt

The next bolt is a *prepared bolt*, that is, one that maintains
in-memory state. In many cases this would mean maintaining a
connection to a database or a queue, or a datastructure aggregating
some aspect of the tuples it processes, but in our example we'll
maintain a complete list of the followers in our system.

This bolt looks more like our spout definition - the second argument
is a list of names, the third argument is a map of bolt configuration
options (importantly, these set +:prepared+ to +true+), and the fourth
argument is the same set of operational arguments we received in
+defspout+:

[source,clojure]
----
(defbolt follow-bolt ["user" "event"] {:prepare true}
  [conf context collector]
----

The body of our bolt first defines the list of followers, and then
provides the actual bolt definition inside a call to +bolt+:

[source,clojure]
----
  (let [follows {:jim #{:rob :emma}
                 :rob #{:karen :kaitlyn :jim}
                 :karen #{:kaitlyn :emma}
                 :kaitlyn #{:jim :rob :karen :kaitlyn :emma :travis}
                 :emma #{:karen}
                 :travis #{:kaitlyn :emma :karen :rob}}]
    (bolt
     (execute [{user "user" event "event" :as tuple}]
              (when ((follows user) (:user event))
                (emit-bolt! collector [user event]))
              (ack! collector tuple)))))
----

Note that the tuple argument is inside the bolts definition of
+execute+ in this case, and may be destructured as usual. In cases
where the event's user is not following the user in the tuple, we do
not emit a new tuple and simply acknowledge that we received our
input.

As we noted earlier, this particular system could be implemented much
more simply be querying whatever datastore tracked follows and simply
adding a story to the feed of each follower. Anticipating a more
complicated system, however, provides a massively extensible
architecture. This bolt could easily be expanded to a collection of
scoring bolts, each of which would evaluate a user/event pair based on
its own criteria and emitting a tuple of (+user+, +event+, +score+). A
score aggregation bolt would receive scores from each scoring bolt and
choose to emit a tuple once it received scores from each type of
scoring bolt in the system. In this world, adjusting the factors
determining the makeup of a user's feed and their relative weights
would be trivial - indeed, production experience with just such a
system was, in the opinion of the authors, delightful.

====== feed-bolt

Our final bolt aggregates events into feeds. Since it only receives
(+user+, +event+) tuples that the "scoring system" has approved it
needs only add the event to the existing list of events it has
received for the given user:

[source,clojure]
----
  (let [feeds (atom {})]
    (bolt
     (execute [{user "user" event "event" :as tuple}]
              (swap! feeds #(update-in % [user] conj event))
              (println "Current feeds:")
              (clojure.pprint/pprint @feeds)
              (ack! collector tuple))))
----

In this case we simply print the current feeds every time we receive a
new event, but in the real world we would save out to a durable
datastore or a cache that could efficiently serve the feeds to our
users.

Note that this design can be easily extended to support event
digesting - rather than storing each event separately we could add an
incoming event to other similar events for our user's convenience.

As currently described, this system has one enormous flaw. By default,
storm tuples are delivered to exactly one instance of each bolt, and
the number of instances in existence is not defined in the bolt
implementation. If the topology operator adds more than one
+feed-bolt+ we may have events for the same user delivered to
different bolt instances, giving each bolt a different feed for the
same user. This may not be a problem if we're simply saving feeds out
to an external service, but if we'd like to do in-memory event
digesting this is enormously problematic.

Happily, this flaw is addressed by the Storm's support for *stream
grouping*, which we define in the Storm topology definition.

====== topology

The topology definition is where the rubber meets the road. Spouts are
wired to bolts are wired to other bolts, and the flow of tuples
between them can be configured to give useful properties to the
computation. It is also where we define the component-level
parallelism of the topology, which provides a rough sketch of the true
operational parallelism of system.

A topology definition consists of spout specifications and bolt
specifications, each of which is a map from names to specifications.

Spout specifications simply give a name to a spout implementation:

[source,clojure]
----
   {"events" (spout-spec event-spout)}
----

Multiple spouts can be configured, and the specification may define
the parallelism of the spout:

[source,clojure]
----
   {
     "events" (spout-spec event-spout)
     "parallel-spout" (spout-spec a-different-more-parallel-spout :p 2)
   }
----

This definition means the topology will have one instance of
+event-spout+ and two instances of +a-different-more-parallel-spout+.

Bolt definitions get a bit more complicated:

[source,clojure]
----
    "active users" (bolt-spec {"spout" :shuffle} active-user-bolt :p 2)
    "follows" (bolt-spec {"active users" :shuffle} follow-bolt :p 2)
----

As with the spout spec we provide a name for this bolt and specify its
parallelism. In addition, bolts require us to specify a *stream grouping*,
which defines (a) which component the bolt receives tuples
from and (b) how the system chooses which instance of the bolt to send
tuples to. In both of these cases we specify :shuffle:, which means
tuples from "events" will be sent to a random instance of
+active-user-bolt+ and tuples from "active users" will be sent to a
random instance of +follow-bolt+.

As noted above, +feed-bolt+ needs to be more careful:

[source,clojure]
----
    "feeds" (bolt-spec {"follows" ["user"]} feed-bolt :p 2)
----

This bolt spec specifies a *fields grouping* on +"user"+. This means
that the all tuples with the same "user" value will be sent to the
same instance of +feed-bolt+. This stream grouping is configured with
a list of field names, so fields groupings may consider the equality
of multiple field values when determining which bolt instance should
process a given tuple.

Storm also supports stream groupings that send tuples to all instances
and groupings that let the bolt that produces a tuple determine
where to send it. Combined with the groupings we've already seen these
provide an enormous amount of flexibility in determining how data
flows through your topology.

Each of these component specifications supports a parallelism option.
Because we have not specified the physical hardware upon which this
topology will run, these hints cannot be used to determine the true
parallelism of the system, but they are used by the cluster to
determine how many in-memory instances of the specified components to
create.

====== deployment

The real magic of Storm comes out in deployment. Storm gives us the
tools us to build small, independent components that make no
assumptions about how many identical instances are running in the same
topology. This means that the topology itself is essentially
infinitely scalable. The edges of the system, where we receive data
from and send data to external components like queues and databases is
not necessarily as scalable, but in many cases strategies for scaling
these services are well understood.

A simple deployment strategy is built into the Storm library:

[source,clojure]
----
  (doto (LocalCluster.)
    (.submitTopology "my first topology"
                     {TOPOLOGY-DEBUG (Boolean/parseBoolean debug)
                      TOPOLOGY-WORKERS (Integer/parseInt workers)}
                     (topology)))
----

+LocalCluster+ is an in-memory implementation of a Storm cluster. We
specify the number of *workers* it will use to execute the
components of our topology and submit the topology itself, at which
point it begins polling the +nextTuple+ methods of the topology's
spouts. As spouts emit tuples, they are propogated through the system
to complete the topology's computation.

Submitting the topology to a configured cluster is nearly as simple,
as we can see in +src/feeds/TopologySubmitter.clj+:

[source,clojure]
----
(defn -main [& {debug "debug" workers "workers" :or {debug "false" workers "4"}}]
  (StormSubmitter/submitTopology
   "feeds topology"
   {TOPOLOGY-DEBUG (Boolean/parseBoolean debug)
    TOPOLOGY-WORKERS (Integer/parseInt workers)}
   (topology)))
----

This file uses Clojure's Java interop to generate a Java class with a
+main+ method. Because our project's +project.clj+ file specifies that
this file should be Ahead Of Time compiled, when we use +lein uberjar+
to build a JAR suitable for submission to our cluster this file will
be compiled to look like a normal Java classfile. We can upload this
JAR to the machine running Storm's *Nimbus* daemon and submit it for
execution using the +storm+ command:

[source,console]
----
$ storm jar path/to/thejariuploaded.jar feeds.TopologySubmitter "workers" 5
----

This command will tell the cluster to allocate 5 dedicated workers for
this topology and begin polling +nextTuple+ on all of its spouts, as
it did when we used +LocalCluster+. A cluster may run any number of
topologies simultaneously - each worker is a physical JVM and may end
up running instances of many different bolts and spouts.

The full details of setting up and running a Storm cluster are out of
the scope of this recipe, but they are documented extensively on
Storm's wiki.

====== Conclusion

We've only touched on a fraction of the functionality Storm has to
offer. Built in Distributed Remote Procedure calls allow users to
harness the power of a the Storm cluster to make synchronous requests
that trigger a flurry of activity across hundreds or thousands of
machines. Guaranteed data processing semantics allow users to build
extremely robust systems. Trident, a higher level abstraction over
Storm's primitives, provides breathtakingly simple solutions to
complicated realtime computing problems. A details runtime console
provides crucial insight into the runtime characteristics of a fully
operational Storm cluster. The power provided by this system is truly
remarkable.

Storm is also a fantastic example of Clojure's ability to be extended
to a problem domain. Its constructs idiomatically extend Clojure
syntax and allow the programmer to stay within the domain of realtime
processing, without needing to deal with low-level language
formalities. This allows Storm to truly "get out of the way" - the
majority of the code in a well written Storm topology's codebase is
focused on the problem at hand. The result is concise, maintainable
code and happy programmers.

===== See also

* http://storm-project.net/[Storm's website]
* https://github.com/travis/lein-storm-project-template[the Storm project template]
* https://github.com/nathanmarz/storm-deploy[the +storm-deploy+ project, for easy Storm deployment]
