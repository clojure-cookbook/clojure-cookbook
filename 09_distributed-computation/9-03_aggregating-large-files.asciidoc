[[sec_aggregating_large_files]]
=== Aggregating Large Files
[role="byline"]
by Alex Robbins

==== Problem

You need to generate aggregate statistics from terabytes of log files.(((distributed computation, aggregating large files)))(((files, aggregating large)))(((Cascalog, aggregating large files with)))(((logging)))(((statistics)))
For example, for a simple input log file (+<date>,<URL>,<USER-ID>+):
----
20130512020202,/,11
20130512020412,/,23
20130512030143,/post/clojure,11
20130512040256,/post/datomic,23
20130512050910,/post/clojure,11
20130512051012,/post/clojure,14
----

you want to output aggregate statistics like this:

----
{
"URL"  {"/"              2
        "/post/datomic"  1
        "/post/clojure"  3}
"User" {"23" 2
        "11" 3
        "14" 1}
"Day"  {"20130512" 6}
}
----

==== Solution

Cascalog allows you to write distributed processing jobs that run
locally or on a Hadoop cluster.

To follow along with this recipe, clone the
http://bit.ly/cc-cascalog-samples[Cascalog samples
GitHub repository] and check out the +aggregation-begin+ branch. This
will give you a basic Cascalog project as created in
<<sec_cascalog_etl>>:

[source,text]
----
$ git clone https://github.com/clojure-cookbook/cascalog-samples.git
$ cd cascalog-samples
$ git checkout aggregation-begin
----

Now add `[cascalog/cascalog-more-taps "2.0.0"]` to the project's
dependencies and set the +cookbook.aggregation+ namespace to be
AOT-compiled. _project.clj_ should look like this:

[source,clojure]
----
(defproject cookbook "0.1.0-SNAPSHOT"
  :description "FIXME: write description"
  :url "http://example.com/FIXME"
  :license {:name "Eclipse Public License"
            :url "http://www.eclipse.org/legal/epl-v10.html"}
  :dependencies [[org.clojure/clojure "1.5.1"]
                 [cascalog "2.0.0"]
                 [cascalog/cascalog-more-taps "2.0.0"]
                 [org.clojure/data.json "0.2.2"]]
  :profiles {:dev {:dependencies [[org.apache.hadoop/hadoop-core "1.1.2"]]}}
  :aot [cookbook.etl
        cookbook.aggregation])
----

Create the file _src/cookbook/aggregation.clj_ and add an aggregation query to it:

[source,clojure]
----
(ns cookbook.aggregation
  (:require [cascalog.api :refer :all]
            [cascalog.more-taps :refer [hfs-delimited]]))

(defn init-aggregate-stats [date url user]
  (let [day (.substring date 0 8)]
    {"URL"  {url 1}
     "User" {user 1}
     "Day"  {date 1}}))

(def combine-aggregate-stats
  (partial merge-with (partial merge-with +)))

(defparallelagg aggregate-stats
  :init-var    #'init-aggregate-stats
  :combine-var #'combine-aggregate-stats)

(defmain Main [in out & args]
  (?<-
    (hfs-textline out :sinkmode :replace)
    [?out]
    ((hfs-delimited in :delimiter ",") ?date ?url ?user)
    (aggregate-stats ?date ?url ?user :> ?out)))
----

Add some sample data to the file _samples/posts/posts.csv_:

----
20130512020202,/,11
20130512020412,/,23
20130512030143,/post/clojure,11
20130512040256,/post/datomic,23
20130512050910,/post/clojure,11
20130512051012,/post/clojure,14
----

[NOTE]
====
The full contents of this solution are available in the
+aggregation-complete+ branch of the
http://bit.ly/cc-cascalog-samples[Cascalog samples]
repository.

Check out that branch to retrieve a full working copy with sample data:

[source,text]
----
$ git checkout aggregation-complete
----
====

You can now execute the job locally:

[source,text]
----
$ lein run -m cookbook.aggregation.Main \
  samples/posts/posts.csv samples/posts/output

# Or, on a Hadoop cluster
$ lein uberjar
$ hadoop jar target/cookbook-standalone.jar \
             cookbook.aggregation.Main \
             samples/posts/posts.csv samples/posts/output
----

The results in _samples/posts/output/part-00000_, formatted for
readability, are as follows:

----
{
"URL"  {"/"              2
        "/post/datomic"  1
        "/post/clojure"  3}
"User" {"23" 2
        "11" 3
        "14" 1}
"Day"  {"20130512" 6}
}
----

==== Discussion

Cascalog makes it easy to quickly generate aggregate
statistics. Aggregate statistics can be tricky on some MapReduce
frameworks. In general, the map phase of a MapReduce job is well
distributed across the cluster. The reduce phase is often less well
distributed. For instance, a naive implementation of the aggregation
algorithm would end up doing all of the aggregation work on a single
reducer. A 2,000-computer cluster would be as slow as a 1-computer
cluster during the reduce phase if all the aggregation happened on one
node.(((aggregate statistics)))

Before you start writing your own aggregator, check through the source
of +cascalog.logic.ops+. This namespace has many useful functions and
probably already does what you want to do.

In our example, the goal is to count occurrences of each URL. To
create the final map, all of the URLs need to end up in one reducer. A
naive MapReduce program implementation would use an aggregation over
all the tuples. That means you'd be doing all the work on only one
node, with the computation taking just as long as it would on a single
computer.

The solution is to use Hadoop's _combiner_ function. Combiners run on
the result of the map phase, before the output is sent to the
reducers. Most importantly, the combiner runs on the mapper
nodes. That means combiner work is spread across the entire cluster,
like map work. When the majority of the work is done during the map
and combiner phases, the reduce phase can run almost
instantly. Cascalog makes this very easy. Many of the built-in
Cascalog functions use combiners under the covers, so you'll be
writing highly optimized queries without even trying. You can even
write your own functions to use combiners using the +defparallelagg+
macro.(((Hadoop, combiner function)))(((macros, defparallelagg)))(((defparallelagg macro)))

[TIP]
====
Cascalog often works with vars instead of the values of those
vars. For example, the call to +defparallelagg+ takes quoted
arguments. The +#'+ syntax means that the var is being passed, not the
value that the var refers to. Cascalog passes the vars around instead
of values so that it doesn't have to serialize functions to pass them
to the mappers and reducers. It just passes the name of the var, which
is looked up in the remote execution environment. This means you won't
be able to dynamically construct functions for some parts of the
Cascalog workflow. Most functions need to be bound to a var.
====

+defparallelagg+ is kind of confusing at first, but the power to write
queries that leverage combiners makes it worth learning. You need to
provide two vars that point to functions to the +defparallelagg+
call: +init-var+ and +combine-var+. Note that both arguments are being
passed as vars, not function values, so you need to prepend a +#'+ to
the names. The +init-var+ function needs to take the input data and
change it into a format that can be easily processed by the
+combine-var+ function. In this case, the recipe changes the data into
a map of maps that can easily be merged. Merging maps is an easy way
to write parallel aggregators. The +combine-var+ function needs to be
commutative and associative. The function is called with two instances
of the output of the +init-var+ function. The return value will be
passed as an argument to later invocations of the +combine-var+
function. Pairs of output will be combined until there is only one
output left, which is the final output.

What follows is an explanation of the query, bit by bit.

First, require the Cascalog functions you'll need:

[source,clojure]
----
(ns cookbook.aggregation
  (:require [cascalog.api :refer :all]
            [cascalog.more-taps :refer [hfs-delimited]]))
----

Then define a function, +init-aggregate-stats+, that takes a date, URL, and
user and returns a map of maps. The second level of maps has keys that
correspond to the observed values. This is the +init+ function, which
takes each row and prepares it for aggregation:

[source,clojure]
----
(defn init-aggregate-stats [date url user]
  (let [day (.substring date 0 8)]
    {"URL"  {url 1}
     "User" {user 1}
     "Day"  {date 1}}))
----

The +combine-aggregate-stats+ function takes the output of invoking
the +init-aggregate-stats+ function on all the inputs and combines
it. This function will be called over and over, combining the output
of +init-aggregate-stats+ function calls and the output of other
invocations of itself. Its output should be of the same form as its
input, since this function will be called on pairs of output until
there is only one piece of data left. This function merges the nested
maps, adding the values together when they are in the same key:

[source,clojure]
----
(def combine-aggregate-stats
  (partial merge-with (partial merge-with +)))
----

+aggregate-stats+ takes the two previous functions and turns them into
a Cascalog parallel-aggregation operation. Note that you pass the
vars, not the functions themselves:

[source,clojure]
----
(defparallelagg aggregate-stats
  :init-var    #'init-aggregate-stats
  :combine-var #'combine-aggregate-stats)
----

Finally, set up +Main+ to define and execute a query that invokes the
+aggregate-stats+ operation across input from +in+, writing it to
+out+:

[source,clojure]
----
(defmain Main [in out & args]
  ;; This defines and executes a Cascalog query.
  (?<-
    ;; Set up the output path.
    (hfs-textline out :sinkmode :replace)
    ;; Define which logic variables will be output.
    [?out]
    ;; Set up the input path, and define the logic vars to bind to input.
    ((hfs-delimited in) ?date ?url ?user)
    ;; Run the aggregation operation.
    (aggregate-stats ?date ?url ?user :> ?out)))
----

If the aggregate you want to calculate can't be defined using
+defparallelagg+, Cascalog provides some other options for defining
aggregates. However, many of them don't use combiners and could leave
you with almost all the computation happening in a small number of
reducers. The computation will probably finish, but you are losing a
lot of the benefit of distributed computation. Check out the source
of +cascalog.logic.ops+ to see what the different options are and how you
can use them.

==== See Also

* The source of
  http://bit.ly/cascalog-ops[+cascalog.logic.ops+],
  a namespace with many predefined operations (including
  aggregators)
