[[sec_aggregating_large_files]]
=== Aggregating Large Files

===== Problem

You need to generate aggregate statistics from terabytes of log
files. For example:

Simple input log file (<date> <URL> <USER-ID>):
----
20130512020202 /             11
20130512020412 /             23
20130512030143 /post/clojure 11
20130512040256 /post/datomic 23
20130512050910 /post/clojure 11
20130512051012 /post/clojure 14
----

Output aggregate statistics:
----
{
"URL"  {"/"              2
        "/post/datomic"  1
        "/post/clojure"  3}
"User" {"23" 2
        "11" 3
        "14" 1}
"Day"  {"20130512" 6}
}
----

===== Solution

Cascalog allows you to write distributed processing jobs that run
locally or on a Hadoop cluster.

Add the following to the dependencies vector of your project.clj:
[source,clojure]
----
[cascalog "1.10.2"]
[cascalog/cascalog-more-taps "1.10.2"]
----

Then add this query to your project:
[source,clojure]
----
(ns cookbook.aggregation
  (:require [cascalog.api :refer :all]
            [cascalog.more-taps :refer [hfs-delimited]]))

(defn init-aggregate-stats [date url user]
  (let [day (.substring date 0 8)]
    {"URL"  {url 1}
     "User" {user 1}
     "Day"  {date 1}}))

(def combine-aggregate-stats
  (partial merge-with (partial merge-with +)))

(defparallelagg aggregate-stats
  :init-var    #'init-aggregate-stats
  :combine-var #'combine-aggregate-stats)

(defmain Main [in out & args]
  (?<-
    (hfs-textline out :sinkmode :replace)
    [?out]
    ((hfs-delimited in) ?date ?url ?user)
    (aggregate-stats ?date ?url ?user :> ?out)))
----

Then execute it locally
[source,terminal]
$ lein run -m cookbook.aggregation.Main <in-path> <out-path>

or execute it over your Hadoop cluster
[source,terminal]
----
$ lein uberjar
$ hadoop jar target/cookbook-standalone.jar cookbook.aggregation.Main <in-path> <out-path>
----

===== Discussion

Cascalog makes it easy to quickly generate aggregate
statistics. Aggregate statistics can be tricky on some MapReduce
frameworks. In general, the map phase of a MapReduce job is well
distributed across the cluster. The reduce phase is often less well
distributed. For instance, a naive implementation of the aggregation
algorithm would end up doing all of the aggregation work on a single
reducer. A 2,000 computer cluster would be as slow as a 1 computer
cluster during the reduce phase if all the aggregation happened on one
node.

Before you start writing your own aggregator, check through the source
of +cascalog.ops+. +cascalog.ops+ has many useful functions and
probably already does what you want to do.

In our example, we need to count how many of each url occurs. To
create the final map, all of the URLs need to end up in one reducer. A
naive MapReduce program implementation would use an aggregation over
all the tuples. That means you'd be doing all the work on only one
node, with the computation taking just as long as it would on a single
computer.

The solution is to use Hadoop's combiner function. Combiners run on
the result of the map phase, before the output is sent to the
reducers. Most importantly, the combiner runs on the mapper
nodes. That means combiner work is spread across the entire cluster,
like map work. When the majority of the work is done during the map
and combiner phases, the reduce phase can run almost
instantly. Cascalog makes this very easy. Many of the built-in
Cascalog functions use combiners under the covers, so you'll be
writing highly-optimized queries without even trying. You can even
write your own functions to use combiners using the +defparallelagg+
macro.

:TIP Cascalog often works with Vars instead of the values of those
Vars. For example, the call to +defparallelagg+ takes quoted
arguments. The #' syntax means that the var is being passed, not the
value that the var refers to. Cascalog passes the vars around instead
of values so that it doesn't have to serialize functions to pass them
to the mappers and reducers. It just passes the name of the var, which
is looked up in the remote execution environment. This means you won't
be able to dynamically construct functions for some parts of the
Cascalog workflow. Most functions need to be bound to a var.

+defparallelagg+ is kind of confusing at first, but the power to write
queries that leverage combiners makes it worth learning. You need to
provide two vars which point to functions to the +defparallelagg+
call: +init-var+ and +combine-var+. Note that both arguments are being
passed as vars, not function values, so you need to prepend a #' to
the name. The +init-var+ function needs to take the input data and
change it into a format that can be easily processed by the
+combine-var+ function. In this case, the recipe changes the data into
a map of maps that can easily be merged. Merging maps is an easy way
to write parallel aggregators. The +combine-var+ function needs to be
commutative and associative. The function is called with two instances
of the output of the +init-var+ function. The return value will be
passed as an argument to later invocations of the +combine-var+
function. Pairs of output will be combined until there is only one
output left, which is the final output.

For example, in this query, the +init-var+ takes the date, url and
user id for each line and transforms them into a nested map with a
value of one for each defined key.  Then +combine-var+ is called with
pairs of nested maps. Combine var just merges the maps, using
+merge-with+ and the addition function. As each new map is combined
in, the counts go up for the defined keys. When all the maps have been
combined, you are left with a single map of maps that contain counts
for each value.

If the aggregate you are wanting to calculate can't be defined using
+defparallelagg+, Cascalog provides some other options for defining
aggregates. However, many of them don't use combiners, and could leave
you with almost all the computation happening in a small number of
reducers. The computation will probably finish, but you are losing a
lot of the benefit of distributed computation. Check out the source
the +cascalog.ops+ to see what the different options are, and how you
can use them.