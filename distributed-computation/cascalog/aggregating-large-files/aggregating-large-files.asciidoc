[[sec_aggregating_large_files]]
=== Aggregating Large Files
[role="byline"]
by Alex Robbins

==== Problem

You need to generate aggregate statistics from terabytes of log
files. For example:

Simple input log file (<date>,<URL>,<USER-ID>):
----
20130512020202,/,11
20130512020412,/,23
20130512030143,/post/clojure,11
20130512040256,/post/datomic,23
20130512050910,/post/clojure,11
20130512051012,/post/clojure,14
----

Output aggregate statistics:
----
{
"URL"  {"/"              2
        "/post/datomic"  1
        "/post/clojure"  3}
"User" {"23" 2
        "11" 3
        "14" 1}
"Day"  {"20130512" 6}
}
----

==== Solution

Cascalog allows you to write distributed processing jobs that run
locally or on a Hadoop cluster.

Add the following to the dependencies vector of your project.clj:
[source,clojure]
----
[cascalog "1.10.2"]
[cascalog/cascalog-more-taps "1.10.2"]
----

Add the Hadoop dependency to your dev profile. This key in your project map
will do it:
[source,clojure]
----
:profiles {:dev {:dependencies [[org.apache.hadoop/hadoop-core "1.1.2"]]}}
----

Set your class to be aot-compiled by adding the aot key to the project
definition.
[source,clojure]
----
:aot [cookbook.aggregation]
----

Then add this query to your project:
[source,clojure]
----
(ns cookbook.aggregation
  (:require [cascalog.api :refer :all]
            [cascalog.more-taps :refer [hfs-delimited]]))

(defn init-aggregate-stats [date url user]
  (let [day (.substring date 0 8)]
    {"URL"  {url 1}
     "User" {user 1}
     "Day"  {date 1}}))

(def combine-aggregate-stats
  (partial merge-with (partial merge-with +)))

(defparallelagg aggregate-stats
  :init-var    #'init-aggregate-stats
  :combine-var #'combine-aggregate-stats)

(defmain Main [in out & args]
  (?<-
    (hfs-textline out :sinkmode :replace)
    [?out]
    ((hfs-delimited in :delimiter ",") ?date ?url ?user)
    (aggregate-stats ?date ?url ?user :> ?out)))
----

Then execute it locally
[source,terminal]
----
$ lein run -m cookbook.aggregation.Main <in-path> <out-path>
----

or execute it over your Hadoop cluster
[source,terminal]
----
$ lein uberjar
$ hadoop jar target/cookbook-standalone.jar cookbook.aggregation.Main <in-path> <out-path>
----

==== Discussion

Cascalog makes it easy to quickly generate aggregate
statistics. Aggregate statistics can be tricky on some MapReduce
frameworks. In general, the map phase of a MapReduce job is well
distributed across the cluster. The reduce phase is often less well
distributed. For instance, a naive implementation of the aggregation
algorithm would end up doing all of the aggregation work on a single
reducer. A 2,000 computer cluster would be as slow as a 1 computer
cluster during the reduce phase if all the aggregation happened on one
node.

Before you start writing your own aggregator, check through the source
of +cascalog.ops+. +cascalog.ops+ has many useful functions and
probably already does what you want to do.

In the example, the goal is to count how many of each url occurs. To
create the final map, all of the URLs need to end up in one reducer. A
naive MapReduce program implementation would use an aggregation over
all the tuples. That means you'd be doing all the work on only one
node, with the computation taking just as long as it would on a single
computer.

The solution is to use Hadoop's combiner function. Combiners run on
the result of the map phase, before the output is sent to the
reducers. Most importantly, the combiner runs on the mapper
nodes. That means combiner work is spread across the entire cluster,
like map work. When the majority of the work is done during the map
and combiner phases, the reduce phase can run almost
instantly. Cascalog makes this very easy. Many of the built-in
Cascalog functions use combiners under the covers, so you'll be
writing highly-optimized queries without even trying. You can even
write your own functions to use combiners using the +defparallelagg+
macro.

:TIP Cascalog often works with Vars instead of the values of those
Vars. For example, the call to +defparallelagg+ takes quoted
arguments. The #' syntax means that the var is being passed, not the
value that the var refers to. Cascalog passes the vars around instead
of values so that it doesn't have to serialize functions to pass them
to the mappers and reducers. It just passes the name of the var, which
is looked up in the remote execution environment. This means you won't
be able to dynamically construct functions for some parts of the
Cascalog workflow. Most functions need to be bound to a var.

+defparallelagg+ is kind of confusing at first, but the power to write
queries that leverage combiners makes it worth learning. You need to
provide two vars which point to functions to the +defparallelagg+
call: +init-var+ and +combine-var+. Note that both arguments are being
passed as vars, not function values, so you need to prepend a #' to
the name. The +init-var+ function needs to take the input data and
change it into a format that can be easily processed by the
+combine-var+ function. In this case, the recipe changes the data into
a map of maps that can easily be merged. Merging maps is an easy way
to write parallel aggregators. The +combine-var+ function needs to be
commutative and associative. The function is called with two instances
of the output of the +init-var+ function. The return value will be
passed as an argument to later invocations of the +combine-var+
function. Pairs of output will be combined until there is only one
output left, which is the final output.

Here is the query, with explanations inline:

Require the Cascalog functions you'll need.
[source,clojure]
----
(ns cookbook.aggregation
  (:require [cascalog.api :refer :all]
            [cascalog.more-taps :refer [hfs-delimited]]))
----

Define a functions that takes a date, url and user and returns a
map of maps. The second level of maps has keys that correspond to
the observed values. This is the init function, which takes each row
and prepares it for aggregation.
[source,clojure]
----
(defn init-aggregate-stats [date url user]
  (let [day (.substring date 0 8)]
    {"URL"  {url 1}
     "User" {user 1}
     "Day"  {date 1}}))
----

This function takes the output of calling the init function on all
the inputs and combines them.  This function will be called over
and over, combining the output of init functions and the output of
other calls of itself. Its output should the same form as its input
since this function will be called on pairs of output until there
is only one piece of data left. This function merges the nested
maps, adding the values together when they are in the same key.
[source,clojure]
----
(def combine-aggregate-stats
  (partial merge-with (partial merge-with +)))
----

This takes the two previous functions and turns them into a Cascalog
parallel aggregation operation. Note that you pass the vars, not the
functions themselves.
[source,clojure]
----
(defparallelagg aggregate-stats
  :init-var    #'init-aggregate-stats
  :combine-var #'combine-aggregate-stats)
----

This makes a new class called Main.
[source,clojure]
----
(defmain Main [in out & args]
  ;; This defines and executes a Cascalog query.
  (?<-
    ;; Setup the output path
    (hfs-textline out :sinkmode :replace)
    ;; Define which logic variables will be output.
    [?out]
    ;; Setup the input path, define the logic vars to bind to input.
    ((hfs-delimited in) ?date ?url ?user)
    ;; Run the aggregation operation.
    (aggregate-stats ?date ?url ?user :> ?out)))
----

If the aggregate you are wanting to calculate can't be defined using
+defparallelagg+, Cascalog provides some other options for defining
aggregates. However, many of them don't use combiners, and could leave
you with almost all the computation happening in a small number of
reducers. The computation will probably finish, but you are losing a
lot of the benefit of distributed computation. Check out the source
the +cascalog.ops+ to see what the different options are, and how you
can use them.
