[[rec_local_io_parallelizing_using_iota]]
=== Parallelizing File Processing with Reducers
[role="byline"]
by Edmund Jackson

==== Problem

You want to use Clojure's reducers on a file to realize parallel
processing without loading the file into memory.((("I/O (input/output) streams", "parallelizing with reducers")))
(((files, parallelizing processing with reducers)))(((Reducers library)))(((Iota library)))

==== Solution

Use the https://github.com/thebusby/iota[Iota] library in
conjunction with the +filter+, +map+, and +fold+ functions from the
Clojure Reducers library in the +clojure.core.reducers+ namespace.((("Clojure", "clojure.core.reducers")))(((functions, filter)))(((functions, map)))(((functions, fold))) To follow along with this recipe, add `[iota "1.1.1"]` to your project's dependencies, or start a REPL with +lein-try+:

[source,text]
----
$ lein try iota
----

To count the words in a very large file, for example:

[source,clojure]
----
(require '[iota                  :as io]
         '[clojure.core.reducers :as r]
         '[clojure.string        :as str])


;; Word-counting functions
(defn count-map
  "Returns a map of words to occurence count in the given string"
  [s]
  (reduce (fn [m w] (update-in m [w] (fnil (partial inc) 0)))
          {}
          (str/split s #" ")))

(defn add-maps
  "Returns a map where each key is the sum of vals of that key in m1 and m2."
  ([] {}) ;; Necessary base case for use as combiner in fold
  ([m1 m2]
     (reduce (fn [m [k v]] (update-in m [k] (fnil (partial + v) 0))) m1 m2)))


;; Main file processing
(defn keyword-count
  "Returns a map of the word counts"
  [filename]
  (->> (iota/seq filename)
       (r/filter identity)
       (r/map count-map)
       (r/fold add-maps)))
----

==== Discussion

The Iota library creates sequences from files on the local filesystem. Unlike the purely sequential lazy sequences produced from
something like +file-seq+, the sequences returned by Iota are
optimized for use with Clojure's Reducers library, which uses the Java
Fork/Join work-stealing frameworkfootnote:[For more information, see
the Java
http://bit.ly/forkjoin-tut[tutorial]
on Fork/Join and work stealing.] under the hood to provide efficient
parallel processing.(((Java, Fork/Join work-stealing framework)))

The +keyword-count+ function first creates a reducible sequence of
lines in the file and filters out blank lines (using the +identity+
function to eliminate +nil+ values from the sequence). Then it applies
the +count-map+ function in parallel, and finally aggregates the
results by folding with the +add-maps+ function.(((functions, keyword)))

+r/filter+ and +r/map+ function exactly the same as their non-Reducer
counterparts; the only difference is one of performance, and how the
Reducers library is able to break down and combine operations. They
also return reducible sequences that can be utilized efficiently by
other operations from the Reducers library.

+r/fold+ is the core function of the Reducers library, and in its
basic form it is functionally very similar to the built-in +reduce+
function. Given a function and a reducible collection, it returns a
value that is the result of applying the folding function to each item
in the collection and an accumulator value.

Unlike with normal +reduce+, however, there is no guaranteed execution
order, which is why +fold+ doesn't take a single starting value as
an argument. It wouldn't make sense, given that the computation can
"start" in several places at once, concurrently. This means that the
function passed to +fold+ (when passed a single function) must also
be capable of taking _zero_ arguments--the result of the no-arg
invocation of the provided function will be used as the seed value for each
branch of the computation.

If you need more flexibility than this provides, +fold+ allows you to
specify both a +reduce+ function and a +combine+ function, as separate
arguments. Exactly what these do is inextricably tied to how
Reducers themselves work, so a full explanation is beyond the scope of
this recipe. See the
http://bit.ly/reducers-fold-doc[API documentation
for the +fold+ function] and the links on the
http://clojure.org/reducers[Reducers page] on Clojure's website for
more information.

===== About Reducers

Reducers is a parallel execution framework for extremely efficient
parallel processing. A full explanation of how reducers work is beyond the scope of this recipe (see http://bit.ly/reducers-post[the blog post] introducing reducers on the Clojure website for a comprehensive treatment).

In short, however, reducers provide performance by two means:

. They can compose operations. Wherever logically possible, the
reducers framework will collapse composable operations into a single
operation. For example, the pass:[<phrase role='keep-together'>preceding</phrase>] code performs a +filter+ and then a
+map+. Clojure's standard +filter+ and +map+ would realize an
intermediate sequence: +filter+ would produce a sequence that would
then be fed to +map+. The reducer versions, however, can compose
themselves (if possible) to produce a single +map+filter+ operation
that can be applied in one shot.

. They exploit the internal tree-like data structures of the data
being reduced. Regular sequences are inherently sequential (no
surprise), and because their performant operation is to pull items
from the beginning one at a time, it's difficult to efficiently
distribute work across their members. However, Reducers is aware of
the internal structure of Clojure's persistent data structures and can
leverage that to efficiently distribute worker processes across the
data.

Under the hood, Iota uses the Java NIO libraries to provide a
_memory-mapped_ view of the file being processed that provides
efficient random access. Iota is also aware of the Reducers framework,
and Iota sequences are structured in such a way that Reducers can
effectively distribute worker processes across them.

==== See Also

* The Iota https://github.com/thebusby/iota[GitHub repository]
* http://bit.ly/javadoc-nio[NIO's documentation]
